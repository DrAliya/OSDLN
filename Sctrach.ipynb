{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac1a4de",
   "metadata": {},
   "source": [
    "# Training a deep neural network (DNN) for sentiment analysis using the Amazon Review Polarity Dataset involves several steps. I'll provide you with a high-level overview of the process. Please note that this is a simplified guide, and the actual implementation can be quite complex, depending on the specific DNN architecture and tools you use. Here are the general steps:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Load the dataset: Read the `train.csv` and `test.csv` files to access the training and testing data.\n",
    "   - Tokenization: Convert the textual data (review titles and review text) into numerical representations, typically using tokenization techniques.\n",
    "   - Padding: Ensure that sequences have the same length by padding or truncating them as necessary.\n",
    "\n",
    "2. **Data Splitting**:\n",
    "   - Split the training dataset into training and validation sets. The validation set is used to tune hyperparameters and monitor training progress.\n",
    "\n",
    "3. **Embedding Layer (Word Vectors)**:\n",
    "   - You may use pre-trained word embeddings like Word2Vec, GloVe, or FastText to represent words in the reviews as dense vectors. Alternatively, you can learn embeddings from scratch as part of your DNN.\n",
    "\n",
    "4. **Model Architecture**:\n",
    "   - Define the architecture of your DNN. A common choice for text classification tasks like sentiment analysis is a recurrent neural network (RNN), a convolutional neural network (CNN), or a combination of both (e.g., LSTM-CNN).\n",
    "\n",
    "5. **DNN Layers**:\n",
    "   - Build the layers of your DNN, which may include embedding layers, recurrent or convolutional layers, pooling layers, and fully connected layers.\n",
    "   - Choose appropriate activation functions, dropout layers, and regularization techniques to prevent overfitting.\n",
    "\n",
    "6. **Loss Function and Optimizer**:\n",
    "   - Define an appropriate loss function for sentiment analysis, such as binary cross-entropy for binary classification.\n",
    "   - Select an optimizer like Adam or SGD to minimize the loss during training.\n",
    "\n",
    "7. **Model Compilation**:\n",
    "   - Compile your DNN model by specifying the loss function, optimizer, and metrics to monitor during training (e.g., accuracy).\n",
    "\n",
    "8. **Training**:\n",
    "   - Train your DNN on the training dataset. Use the validation set to monitor the model's performance and avoid overfitting.\n",
    "   - Experiment with different hyperparameters like learning rate, batch size, and the number of epochs to find the best settings.\n",
    "\n",
    "9. **Evaluation**:\n",
    "   - Evaluate the trained model on the test dataset to assess its performance on unseen data.\n",
    "   - Calculate metrics such as accuracy, precision, recall, and F1-score to measure classification performance.\n",
    "\n",
    "10. **Fine-Tuning** (Optional):\n",
    "    - Depending on the evaluation results, you may fine-tune your model by adjusting hyperparameters or experimenting with different architectures.\n",
    "\n",
    "11. **Inference**:\n",
    "    - Use the trained DNN model for making predictions on new, unseen reviews for sentiment analysis.\n",
    "\n",
    "12. **Deployment** (if applicable):\n",
    "    - If you plan to deploy the model in a production environment, integrate it with your application or service for real-time predictions.\n",
    "\n",
    "Remember that building an effective sentiment analysis model can be a complex task, and you may need to iterate on the above steps and experiment with different architectures and hyperparameters to achieve the best results. Additionally, libraries and frameworks like TensorFlow, PyTorch, and Keras are commonly used for implementing deep neural networks in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0365d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc2a9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 11:49:05.558651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a65890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40f210",
   "metadata": {},
   "source": [
    "    Read the Data from the Files:\n",
    "        First, you need to read the text data from the Bzip2 files you've loaded into train_file and test_file. You can use the .read() method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e37cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the Bzip2 files\n",
    "train_data = train_file.read().decode('utf-8')\n",
    "test_data = test_file.read().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416252cf",
   "metadata": {},
   "source": [
    "Depending on your dataset, you may want to split the text into sentences or paragraphs before tokenization. You can use libraries like NLTK or spaCy for sentence splitting if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867addc",
   "metadata": {},
   "source": [
    "Tokenization with Regular Expressions (Optional):\n",
    "\n",
    "    You can use regular expressions to split the text into tokens based on spaces, punctuation, or other delimiters. Here's an example using Python's re module to split text into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f226c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using regular expressions (split by whitespace)\n",
    "train_tokens = re.split(r'\\s+', train_data)\n",
    "test_tokens = re.split(r'\\s+', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743100ed",
   "metadata": {},
   "source": [
    "Tokenization with Libraries (Recommended):\n",
    "\n",
    "    While basic tokenization using regular expressions works, it's often better to use specialized NLP libraries like spaCy or NLTK for more robust tokenization. These libraries can handle complex tokenization scenarios, including handling punctuation, contractions, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d177519c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the spaCy English tokenizer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize the text using spaCy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_doc \u001b[38;5;241m=\u001b[39m nlp(train_data)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Tensorflow_env/lib/python3.9/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Tensorflow_env/lib/python3.9/site-packages/spacy/util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English tokenizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenize the text using spaCy\n",
    "train_doc = nlp(train_data)\n",
    "test_doc = nlp(test_data)\n",
    "\n",
    "# Extract tokens from the spaCy Doc objects\n",
    "train_tokens = [token.text for token in train_doc]\n",
    "test_tokens = [token.text for token in test_doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35896c84",
   "metadata": {},
   "source": [
    "Now, train_tokens and test_tokens contain the tokenized versions of your training and testing data, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7c78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

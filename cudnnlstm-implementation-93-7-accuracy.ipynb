{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0a2b61717e468b5a6b9820240af45236260c7168"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 11:35:31.523448: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "400fa34d9a714a22a20259ba3140a030551e1986"
   },
   "source": [
    "# Read Train & Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df4ce91d1b19fe5c93e401a3ce64fd4d1f07787a"
   },
   "source": [
    "# Create Lists containing Train & Test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "d554f2ba-0eeb-4764-b1df-febfbdda5c2c",
    "_uuid": "93213b70d17edcdeeba304a7344e415733d0ce17"
   },
   "outputs": [],
   "source": [
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4a67bd8e9d198026184e07764feeaa8727288e64"
   },
   "outputs": [],
   "source": [
    "del train_file, test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0357f1f5175f121b2d2435fd7e2477f4b2a0e17e"
   },
   "source": [
    "# Convert from raw binary strings to strings that can be parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "ceda406d-42f3-46de-930c-f658b1090792",
    "_uuid": "5758abb69d2f0b7434041d5444916089ef468d88"
   },
   "outputs": [],
   "source": [
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "dcc69630-91ef-4a06-9eb4-56c4b0390a49",
    "_uuid": "539afb8a439f11442ad8ca5c1de042a3a310462f"
   },
   "outputs": [],
   "source": [
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "                                                       \n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "a1b5fb60ceafd2f0776af45a4722fc55c15903c5"
   },
   "outputs": [],
   "source": [
    "del train_file_lines, test_file_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "03b49435-b06b-4f72-b8f2-27faae8cf478",
    "_uuid": "3754d4d288fe5411ee0cc26112a17dd34a6e427c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "0ab720adb979acbb71bdbc1b0baf975b592049ea"
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "7fafbeb9c7604d9306ee0b11831769b616bb3b49"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c9ebc89a3e784567de70e9b77b514e0553e75a43"
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "a228c36b8fad3cf02e19cbb43210bb10735807a2"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/Users/zaarr/opt/anaconda3/envs/Tensorflow_env/lib/python3.9/site-packages/keras/preprocessing/sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Your code to tokenize train_sentences\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(train_sentences)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/Users/zaarr/opt/anaconda3/envs/Tensorflow_env/lib/python3.9/site-packages/keras/preprocessing/sequence.py)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Your code to tokenize train_sentences\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "# Apply pad_sequences to your tokenized data\n",
    "X_train = pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "\n",
    "\n",
    "#tokenized_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "#X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d129cf469aab9863fe98e96eae08c669547377d"
   },
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29ff082b690cc6ef49136732bc12e6adf95ef693"
   },
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c367ba4af4d31d0af1a350bef7342765f979890d"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/glovetwitter100d/glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03da96d2465ae82506564b2179e44bc4dce7e6f0"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d80da73ef79b38fd7dde68cacc3b19df5f4922d9"
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91329150363b5ff70ce494c6fabd1b68ffbb7b8c"
   },
   "outputs": [],
   "source": [
    "del tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences, word_index, embeddings_index, all_embs, nb_words\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
   },
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "epochs = 7\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1c413e9230e1b81c2082b651cd2e90fa9a6bfaf"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c36d39a2094cbaa7e306af22cd870e986f473ff"
   },
   "outputs": [],
   "source": [
    "def cudnnlstm_model(conv_layers = 2, max_dilation_rate = 3):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    prefilt = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    x = prefilt\n",
    "    for strides in [1, 1, 2]:\n",
    "        x = Conv1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x_f = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n",
    "    x_b = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x = concatenate([x_f, x_b])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "cudnnlstm_model = cudnnlstm_model()\n",
    "cudnnlstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cc9437fc6b557e4fcfca155abb10c00e0935798"
   },
   "outputs": [],
   "source": [
    "weight_path=\"early_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
   },
   "outputs": [],
   "source": [
    "cudnnlstm_model.fit(X_train, train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6197ef311e5644424397a66fb5dd287f9040d4c3"
   },
   "outputs": [],
   "source": [
    "cudnnlstm_model.load_weights(weight_path)\n",
    "score, acc = cudnnlstm_model.evaluate(X_test, test_labels, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61d38989f67f28214873139c0c315d9bfe08df79"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
